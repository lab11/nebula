# Simulation Data using Probabilistic Routing

Instead of using raw mobility traces in our simulations, we want to have some concept of expected traffic flow and instantiate an arbitrary number of mules and sensors that interact accordingly. Thus, we construct a heatmap and generate trajectories using randomized starting and ending positions with a randomly generated trajectory in between the two. Each step in the trajectory is weighted by values in the traffic heatmap and biased to continue in the same direction while making progress towards the target destination.

The creation of the heatmap, generation of weighted random walks, and collection of simulation data is done in `probabilistic_trajectories.ipynb`. The last section of the notebook, which is titled "Collect simulation data", is stand-alone and can be modified and run to generate additional simulation results without running the entirety of the notebook.

## Simulation setup and data format

We ran the simulation with 100 mobile mules and 1000 stationary sensors over one hour of time. Each mule has its own speed uniformly randomly selected to be between 1 and 15 meters per second, and each sensor has a range uniformly randomly selected to be between 10 and 20 meters. We ignore WiFi hotspots for now and assume that the mule is able to upload from anywhere using a cellular data plan. Simulation results are stored in three different files, described below.

1. `prob_data/interactions.csv` holds the results of the simulation. We want to collect interaction data and some concept of expected traffic flow. The collected data has the following format:

| sensor_id | mule_id | interaction_time | interaction_duration |
| :---: | :---: | :---: | :---: |
| i = 1, ..., 1000 | j = 1, ..., 100 | seconds | seconds |

where `interaction_time` is the time in seconds when the mule walks within range of the sensor and `interaction_duration` is the amount of time the mule spends within the range of the sensor before leaving.

2. `prob_data/sensor_metadata.csv` contains some metadata on the sensors, which were uniformly randomly deployed across the available area. The collected data has the following format:

| sensor_id | x | y | radius | traffic |
| :---: | :---: | :---: | :---: | :---: |
| i = 1, ..., 1000 | x = \[0, 800\] | y = \[0, 950\] | r = \[10, 20\] | float |

where `x, y, r` is in meters and `traffic` is the value of the heatmap in the position that the sensor is at.

3. `prob_data/mule_metadata.csv` contains some metadata on the mules and has the following format:

| mule_id | speed | displacement |
| :---: | :---: | :---: |
| j = 1, ..., 100 | s = \[1, 15\] | meters | 

where `s` is in meters per second and `displacement` is the Euclidean distance between the mule's randomly selected start and target positions. The purpose of `displacement` is to provide some information about the mule's trajectory without needing to record the actual trajectory for each mule.

## Latency calculations

Based on the generated interaction information, we can get an estimate for the average maximum latency experienced by each sensor, where latency measures the time between each successive mule interaction. We calculate these values in `latency_calculations.ipynb`. 

Plotting latency estimates against expected traffic (i.e. heatmap values) gives us a downward trend, where latency decreases as expected traffic increases. This confirms that the random walks generated by our simulation generally conform to the underlying heatmap values, which is good. 

Plotting latency estimates against number of mules also gives us a downward trend, which confirms the intuition that as mule saturation increases, the latency of data collection decreases.

## Simulated upload schedule

Using the interaction information, we can also generate an upload schedule for data backhaul. We do this in `schedule_generation.ipynb`. We set a number of parameters to constrain the schedule:

- `number of mules` - number (integer) of mules that are included in the simulation
- `number of sensors` - number (integer) of sensors that are included in the simulation
- `advertisement period` - time (in seconds) between each sensor's BLE advertisement used to discover nearby mules
- `connection time` - time (in seconds) needed for a connection to form before data can be transferred
- `ble throughput` - rate (in bytes per second) of data transfer from a sensor to a mule
- `sample period` - time (in seconds) between each sample that a sensor takes
- `sample length` - size (in bytes) of the samples a sensor transfers to a mule
- `batch period` - time (in seconds) between each batch the mule uploads
- `batch length` - size (in bytes) of the batches a mule uploads to the cloud

We then walk through the sensor-mule interactions and generate upload data in `prob_data/schedule.csv` with the following format:

| sensor_id | mule_id | sample_time | pickup_time | batch_time | data_length |
|:--:|:--:|:--:|:--:|:--:|:--:|
| i = 0, ..., 999 | j = 0, ..., 99 | seconds | seconds | seconds | bytes |

where `sample_time` is the time that the sensor generated the sample, `pickup_time` is the time that the mule walked into the sensor's range and picked up the packet (which is also the upload time for a non-privacy-preserving setup), and `batch_time` is the time that the packet would be uploaded if it the upload time is delayed and uploads are batched into constant sized chunks for privacy reasons.

We assume that each sensor connects to only one mule at a time, while a mule can connect to an arbitrary number of sensors at a time. After a sensor connects to a mule, it stays connected and transfers data as long as the mule is within range. Once the mule leaves the sensor's range, the sensor immediately starts advertisements and looks for a new mule to form a connection with.
