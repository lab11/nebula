{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule generation\n",
    "\n",
    "Now that we have simulation data on the interactions between sensors and mules, we want to generate an upload schedule based on sampling frequency and batched updates.\n",
    "\n",
    "We will collect the following thingomaboobers:\n",
    "\n",
    "`schedule.csv`\n",
    "\n",
    "| sensor_id | mule_id | sample_time | pickup_time | batch_time | data_length |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| i = 0, ..., 999 | j = 0, ..., 99 | seconds | seconds | seconds | bytes |\n",
    "\n",
    "where `sample_time` is the time that the sensor generated the sample, `pickup_time` is the time that the mule walked into the sensor's range and picked up the packet (which is also the upload time for a non-privacy-preserving setup), and `batch_time` is the time that the packet would be uploaded if it the upload time is delayed and uploads are batched into constant sized chunks for privacy reasons. \n",
    "\n",
    "We will also have the following parameters that can be twiggled:\n",
    "\n",
    "- `number of mules` - number (integer) of mules that are included in the simulation\n",
    "- `number of sensors` - number (integer) of sensors that are included in the simulation\n",
    "- `advertisement period` - time (in seconds) between each sensor's BLE advertisement used to discover nearby mules\n",
    "- `connection time` - time (in seconds) needed for a connection to form before data can be transferred\n",
    "- `ble throughput` - rate (in bytes per second) of data transfer from a sensor to a mule\n",
    "- `sample period` - time (in seconds) between each sample that a sensor takes\n",
    "- `sample length` - size (in bytes) of the samples a sensor transfers to a mule\n",
    "- `batch period mean` - average time (in seconds) between each batch the mule uploads\n",
    "- `batch period std` - deviation in time (in seconds) between each batch the mule uploads \n",
    "- `batch period min` - minimum time (in seconds) between each batch the mule uploads\n",
    "- `batch length` - size (in bytes) of the batches a mule uploads to the cloud\n",
    "\n",
    "I think this is all we need for now to evaluate the baseline and Express. We assume that each sensor connects to only one mule at a time, while a mule can connect to an arbitrary number of sensors at a time. After a sensor connects to a mule, it stays connected and transfers data as long as the mule is within range. Once the mule leaves the sensor's range, the sensor immediately starts advertisements and looks for a new mule to form a connection with.\n",
    "\n",
    "## To generate new data using this notebook\n",
    "\n",
    "Either run this notebook and input different parameters into the `generate_schedule` function, or import the function into another Python script to use it in your own code. This should suffice for basic trials.\n",
    "\n",
    "Currently, we assume that all sensors have the same advertisement period, connection time, BLE throughput, sample period, and sample length, and that all mules have the same batch length. To make these values differ across sensors and mules or to have them change over time, you will have to make some small changes to the code for value generation and proper bookkeeping. This should be reasonably straightforward.\n",
    "\n",
    "We also assume that each mule can connect to an arbitrary number of sensors and store an arbitrary amount of data collected from sensors. This code does not support any connection or memory restrictions on the mule's side. If you want to consider the case where each mule can only connect to one sensor at a time or only accepts up to, say, 10KB of data at any time and shuts down all BLE connections after hitting that limit, you will have to make major adjustments to how the schedule is generated, and potentially rewrite substantial portions of the code. This is because we currently generate all traffic outward from sensors to mules (unaware of the mules' internal states) before retroactively batching all of the uploads for each mule. This allows us to consider each sensor's outward traffic independent of all other sensors. However, a connection or memory limit for mules would conflate the traffic from multiple sensors sending packets to the same mule, preventing us from considering each sensor individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Set a seed for our stochastic uploads \n",
    "np.random.seed(1337)\n",
    "\n",
    "# Toss everything into a function for looping purposes\n",
    "def generate_schedule(num_mules = 100, # integer <= 100\n",
    "                      num_sensors = 1000, # integer <= 1000\n",
    "                      advertisement_period = 2.0, # seconds\n",
    "                      connection_time = 1.5, # seconds\n",
    "                      ble_throughput = 125000.0, # bytes per second\n",
    "                      sample_period = 10.0, # seconds\n",
    "                      sample_length = 128, # bytes\n",
    "                      batch_period_min = 60.0, # seconds\n",
    "                      batch_period_max = 600.0, # seconds\n",
    "                      batch_length = 100000, # bytes\n",
    "                      interaction_file = 'prob_data/continual_motion/interactions.csv', # csv file path\n",
    "                      save_file = 'prob_data/random_uploads/schedule.csv', # csv file path\n",
    "                      verbose = False\n",
    "                     ):\n",
    "    \n",
    "    if verbose:\n",
    "        print('Reading interactions from `{}`.'.format(interaction_file))\n",
    "        print('Using {} sensors and {} mules.'.format(num_sensors, num_mules))\n",
    "        print('Mules upload every {} seconds on average.'.format((batch_period_min + batch_period_max)/2.0))\n",
    "    \n",
    "    # Read in data files and downsample as necessary.\n",
    "    interaction_df = pd.read_csv(interaction_file)\n",
    "    smol_interactions = interaction_df.loc[(interaction_df['sensor_id'] < num_sensors) & \n",
    "                                           (interaction_df['mule_id'] < num_mules)]\n",
    "    # Sort the interactions by sample ID and interaction time.\n",
    "    smol_interactions = smol_interactions.sort_values(['sensor_id', 'interaction_time'])\n",
    "\n",
    "    # Calculate some useful numbers.\n",
    "    time_per_sample = sample_length / ble_throughput \n",
    "    samples_per_batch = math.floor(batch_length / sample_length)\n",
    "\n",
    "    # Start bookkeeping.\n",
    "    cur_sensor = 0 # Keeps track of the sensor we are generating a schedule for.\n",
    "    next_sample = 0 # Accumulates samples for each sensor to send.\n",
    "    cur_end_time = 0.0 # Keeps track of the latest action time for each sensor.\n",
    "    schedule = [] # Records our resulting upload schedule.\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print('Generating sensor-to-mule data transfers...')\n",
    "    \n",
    "    # Iterate through each row of sensor-mule interactions.\n",
    "    for index, row in smol_interactions.iterrows():\n",
    "        # Grab the sensor and mule used in this interaction.\n",
    "        new_sensor = int(row['sensor_id'])\n",
    "        new_mule = int(row['mule_id'])\n",
    "\n",
    "        # If we moved on to the next sensor, reset our bookkeeping.\n",
    "        if cur_sensor != new_sensor:\n",
    "            cur_sensor = new_sensor\n",
    "            next_sample = 0\n",
    "            cur_end_time = 0.0\n",
    "\n",
    "        # If the sensor has already taken actions beyond this time, ignore this row.\n",
    "        new_end_time = row['interaction_time'] + row['interaction_duration']\n",
    "        if cur_end_time >= new_end_time:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, we advertise and attempt to start a connection.\n",
    "        time_passed = max(0.0, row['interaction_time'] - cur_end_time)\n",
    "        new_start_time = cur_end_time + math.ceil(time_passed / advertisement_period) * advertisement_period\n",
    "        # If there is not enough time for a connection, we waste some time and move on with our lives.\n",
    "        cur_end_time = new_start_time + connection_time\n",
    "        if cur_end_time >= new_end_time:\n",
    "            cur_end_time = new_end_time\n",
    "            continue\n",
    "        # Otherwise, we successfully connected to the mule and have time to do stuff.\n",
    "\n",
    "        # The first thing we do is dump a bunch of accumulated samples onto the mule as fast as possible\n",
    "        # as long as the mule is connected.\n",
    "        while next_sample * sample_period <= cur_end_time and cur_end_time + time_per_sample <= new_end_time:\n",
    "            cur_end_time += time_per_sample\n",
    "            # Send the sample with placeholders for batching.\n",
    "            schedule.append([new_sensor,                  # sensor_id\n",
    "                             new_mule,                    # mule_id \n",
    "                             next_sample * sample_period, # sample_time\n",
    "                             cur_end_time,                # pickup_time\n",
    "                             -1,                          # batch_time\n",
    "                             sample_length])              # data_length\n",
    "            next_sample += 1\n",
    "\n",
    "        # After we have done that, we continue the connection and send new samples as they come in.\n",
    "        while next_sample * sample_period + time_per_sample <= new_end_time:\n",
    "            # Send the sample with placeholders for batching.\n",
    "            schedule.append([new_sensor,                                    # sensor_id\n",
    "                             new_mule,                                      # mule_id \n",
    "                             next_sample * sample_period,                   # sample_time\n",
    "                             next_sample * sample_period + time_per_sample, # pickup_time\n",
    "                             -1,                                            # batch_time\n",
    "                             sample_length])                                # data_length\n",
    "            next_sample += 1\n",
    "\n",
    "        # Finally, once we are done sending all the samples that can be sent, we close out the interaction\n",
    "        # and update the bookkeeping as necessary.\n",
    "        cur_end_time = new_end_time\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print('Generating mule batched upload times...')\n",
    "    \n",
    "    # Now we want to figure out which batches the mule will end up sending each sample in.\n",
    "    # We do this based on chronological order of sample receipt. \n",
    "    labels = ['sensor_id', 'mule_id', 'sample_time', 'pickup_time', 'batch_time', 'data_length']\n",
    "    schedule = pd.DataFrame(schedule, columns=labels)\n",
    "    schedule = schedule.sort_values(['mule_id', 'pickup_time'])\n",
    "\n",
    "    # Setup some bookkeeping.\n",
    "    cur_mule = 0\n",
    "    next_batch_time = 0.0\n",
    "    next_batch_length = 0\n",
    "\n",
    "    # Iterate through each row of sensor-mule packet transfers.\n",
    "    for index, row in schedule.iterrows():\n",
    "        # Grab some useful values.\n",
    "        new_mule = int(row['mule_id'])\n",
    "        pickup_time = row['pickup_time']\n",
    "\n",
    "        # If we are looking at a new mule, reset our bookkeeping.\n",
    "        if cur_mule != new_mule:\n",
    "            cur_mule = new_mule\n",
    "            next_batch_time = 0.0\n",
    "            next_batch_length = 0\n",
    "\n",
    "        # If we already sent the next batch, increment our batch index.\n",
    "        while next_batch_time <= pickup_time:\n",
    "            next_batch_time += np.random.uniform(batch_period_min, batch_period_max)\n",
    "            next_batch_length = 0\n",
    "\n",
    "        # Increment the number of samples in this upcoming batch. If there are too many, we postpone\n",
    "        # this sample until the next batch.\n",
    "        next_batch_length += 1\n",
    "        if next_batch_length > samples_per_batch:\n",
    "            next_batch_time += np.random.uniform(batch_period_min, batch_period_max)\n",
    "            next_batch_length = 1\n",
    "\n",
    "        # Update the batch upload time.\n",
    "        schedule.at[index, 'batch_time'] = next_batch_time\n",
    "        \n",
    "        \n",
    "    # Save our results\n",
    "    with open(save_file, 'w') as f:\n",
    "        # Record parameters.\n",
    "        f.write('num_mules,num_sensors,advertisement_period,connection_time,ble_throughput,sample_period,sample_length,batch_period_min,batch_period_max,batch_length\\n')\n",
    "        f.write('{},{},{},{},{},{},{},{},{}\\n\\n'.format(num_mules,num_sensors,advertisement_period,connection_time,ble_throughput,sample_period,sample_length,batch_period_min,batch_period_max,batch_length))\n",
    "\n",
    "        # Record schedule.\n",
    "        schedule.to_csv(f, index=False)\n",
    "\n",
    "        \n",
    "    # Quick printout for funsies.\n",
    "    if verbose:\n",
    "        print('Saved results to {}.'.format(save_file))\n",
    "        print(schedule.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading interactions from `prob_data/continual_motion/interactions.csv`.\n",
      "Using 1000 sensors and 100 mules.\n",
      "Mules upload every 180.0 seconds on average.\n",
      "Generating sensor-to-mule data transfers...\n",
      "Generating mule batched upload times...\n",
      "Saved results to prob_data/random_uploads/schedule.csv.\n",
      "        sensor_id  mule_id  sample_time  pickup_time  batch_time  data_length\n",
      "184214        697        0          0.0     1.501024         122          128\n",
      "261649        977        0          0.0     1.501024         122          128\n",
      "166797        624        0          0.0     3.501024         122          128\n",
      "170615        638        0          0.0     7.501024         122          128\n",
      "3905           17        0         20.0    20.001024         122          128\n"
     ]
    }
   ],
   "source": [
    "# Try out our function\n",
    "generate_schedule(batch_period_max=300.0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate schedules for different numbers of mules\n",
    "\n",
    "Now that the schedule generator is working, let's spit out a few traces for variable number of mules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters.\n",
    "num_mules_list = [10, 20, 30, 40, 50, 60, 70 ,80, 90, 100] # integer <= 100\n",
    "\n",
    "# Set save file.\n",
    "save_file = 'prob_data/random_uploads/vary_mules/{}_mule_schedule.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating schedule for 10 mules\n",
      "Finished generating schedule for 20 mules\n",
      "Finished generating schedule for 30 mules\n",
      "Finished generating schedule for 40 mules\n",
      "Finished generating schedule for 50 mules\n",
      "Finished generating schedule for 60 mules\n",
      "Finished generating schedule for 70 mules\n",
      "Finished generating schedule for 80 mules\n",
      "Finished generating schedule for 90 mules\n",
      "Finished generating schedule for 100 mules\n"
     ]
    }
   ],
   "source": [
    "for num_mules in num_mules_list:\n",
    "    generate_schedule(num_mules=num_mules, save_file=save_file.format(num_mules))\n",
    "    print(\"Finished generating schedule for {} mules\".format(num_mules))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
